{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-07T06:18:24.473362Z",
     "start_time": "2025-03-07T06:18:14.767767Z"
    }
   },
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime, timedelta\n",
    "from xgboost import XGBRegressor\n",
    "import plotly.express as px\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import holidays\n",
    "import warnings\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "def upload_data():\n",
    "    data_path = str(Path.cwd().parent) + \"\\\\Data\\\\EPC\\\\Power Consumption Data.csv\"\n",
    "    \n",
    "    df = pd.read_csv(data_path)\n",
    "    \n",
    "    df = df[df[\"real_consumption\"] > 0]\n",
    "    df = df[df['real_consumption'] <= df['real_consumption'].mean() + 4 * df['real_consumption'].std()]\n",
    "    \n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df = df.sort_values(by='time',ascending=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def data_metrics(data, real, predicted):\n",
    "\n",
    "    y_true = data[real]\n",
    "    y_pred = data[predicted]\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "    # MAE (Mean Absolute Error):\n",
    "    # Lower values are better; good MAE depends on the scale of 'real_consumption'.\n",
    "    # As a rule of thumb, MAE should be significantly smaller than the mean of the target variable.\n",
    "    # Lower is better. Ideally, MAE should be much less than the average value of y_true.\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "\n",
    "    # MSE (Mean Squared Error):\n",
    "    # Similar to MAE but penalizes large errors more heavily. A smaller MSE is better.\n",
    "    # Compare MSE to the variance of 'real_consumption' for context.\n",
    "    # Lower is better. MSE should ideally be close to zero relative to the variance of y_true.\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "    # RMSE (Root Mean Squared Error):\n",
    "    # RMSE is the square root of MSE and is in the same units as 'real_consumption'.\n",
    "    # A good RMSE is often close to the standard deviation of 'real_consumption'.\n",
    "    # Lower is better. RMSE should be comparable to or less than the standard deviation of y_true.\"\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "    # R² (Coefficient of Determination):\n",
    "    # R² measures how well the predictions explain the variability of the data.\n",
    "    # Values close to 1.0 are excellent, indicating the model explains most of the variance.\n",
    "    # Negative values indicate poor fit.\n",
    "    # Closer to 1.0 is better. Values > 0.7 are generally good; < 0.5 indicates underfitting.\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "\n",
    "def feature_engineering(data):\n",
    "\n",
    "    # Extracting basic time-based features\n",
    "    data['hour'] = data['time'].dt.hour  # Hour of the day\n",
    "    data['minute'] = data['time'].dt.minute  # Minute\n",
    "    data['day_of_week'] = data['time'].dt.dayofweek  # Day of the week (0=Monday, 6=Sunday)\n",
    "    data['is_weekend'] = data['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)  # Weekend flag\n",
    "    data['day_of_month'] = data['time'].dt.day\n",
    "    data['week_of_year'] = data['time'].dt.isocalendar().week\n",
    "    data['month'] = data['time'].dt.month\n",
    "    data['quarter'] = data['time'].dt.quarter\n",
    "    data['year'] = data['time'].dt.year\n",
    "\n",
    "\n",
    "    # Generate lag features for temporal dependency modeling\n",
    "    for lag in range(1, 5):  # Create lag features for the past 4 time steps\n",
    "        data[f'lag_{lag}'] = data['real_consumption'].shift(lag)\n",
    "\n",
    "\n",
    "    # Generate exponential moving averages\n",
    "    for span in [3, 5]:  # Spans of size 3, 5, and 7\n",
    "        data[f'ema_{span}'] = data['real_consumption'].ewm(span=span, adjust=False).mean()\n",
    "\n",
    "    # Rolling average over a longer period (e.g., weekly and monthly moving averages)\n",
    "    data['weekly_avg'] = data['real_consumption'].rolling(window=7*24*20, min_periods=1).mean()  # Weekly moving avg\n",
    "    data['monthly_avg'] = data['real_consumption'].rolling(window=30*24*20, min_periods=1).mean()  # Monthly moving avg\n",
    "\n",
    "\n",
    "# Percentage change in real consumption\n",
    "    data['pct_change'] = data['real_consumption'].pct_change()\n",
    "\n",
    "    data['hour_sin'] = np.sin(2 * np.pi * data['hour'] / 24)  # Cyclic hour feature (sine)\n",
    "    data['hour_cos'] = np.cos(2 * np.pi * data['hour'] / 24)  # Cyclic hour feature (cosine)\n",
    " \n",
    "    data['day_of_week_sin'] = np.sin(2 * np.pi * data['day_of_week'] / 7)\n",
    "    data['day_of_week_cos'] = np.cos(2 * np.pi * data['day_of_week'] / 7)\n",
    "    \n",
    "    data['month_sin'] = np.sin(2 * np.pi * data['month'] / 12)\n",
    "    data['month_cos'] = np.cos(2 * np.pi * data['month'] / 12)\n",
    "    \n",
    "    data['week_of_year_sin'] = np.sin(2 * np.pi * data['week_of_year'] / 52)\n",
    "    data['week_of_year_cos'] = np.cos(2 * np.pi * data['week_of_year'] / 52)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Get Georgia holidays for all years in the dataset\n",
    "    georgia_holidays = holidays.Georgia(years=range(data[\"year\"].min(), data[\"year\"].max() + 1))\n",
    "\n",
    "    data[\"date\"] = data[\"time\"].dt.date\n",
    "    \n",
    "    # Create holiday feature (1 if it's a holiday, 0 otherwise)\n",
    "    data['is_holiday'] = data[\"date\"].map(lambda x: 1 if x in georgia_holidays else 0)\n",
    "    \n",
    "    # Add features for the day before and after a holiday\n",
    "    data['is_day_before_holiday'] = data[\"date\"].map(lambda x: 1 if (x - pd.Timedelta(days=1)) in georgia_holidays else 0)\n",
    "    data['is_day_after_holiday'] = data[\"date\"].map(lambda x: 1 if (x + pd.Timedelta(days=1)) in georgia_holidays else 0)\n",
    "\n",
    "\n",
    "\n",
    "    return  data\n",
    "\n"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T06:18:24.511360Z",
     "start_time": "2025-03-07T06:18:24.483370Z"
    }
   },
   "id": "6b1e95fb5f8b736d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "df = upload_data()\n",
    "\n",
    "data_metrics(data=df, real=\"real_consumption\", predicted=\"predicted_consumption\")\n",
    "\n",
    "df = feature_engineering(df)[[\"time\",\"real_consumption\",\"predicted_consumption\"]]\n"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T06:18:39.442133Z",
     "start_time": "2025-03-07T06:18:24.681301Z"
    }
   },
   "id": "5c638f67c9e70eca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 56.7256\n",
      "MSE: 5824.3342\n",
      "RMSE: 76.3173\n",
      "R²: 0.9176\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T06:18:39.480387Z",
     "start_time": "2025-03-07T06:18:39.474378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "scaler = MinMaxScaler()\n",
    "# df[\"real_consumption\"] = scaler.fit_transform(df[\"real_consumption\"].values.reshape(-1, 1))\n",
    "# df[\"predicted_consumption\"] = scaler.fit_transform(df[\"predicted_consumption\"].values.reshape(-1, 1))"
   ],
   "id": "839af60ffab8fd66",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T06:18:42.065494Z",
     "start_time": "2025-03-07T06:18:39.502254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SEQ_LEN = 30  # Use past 12 hours (240 steps, 3-minute intervals)\n",
    "PRED_LEN = 3  # Predict next 12 hours\n",
    "\n",
    "def create_sequences(data, seq_len, pred_len):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_len - pred_len):\n",
    "        X.append(data[i : i + seq_len])\n",
    "        y.append(data[i + seq_len : i + seq_len + pred_len])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Prepare sequences\n",
    "X, y = create_sequences(df[\"real_consumption\"].values, SEQ_LEN, PRED_LEN)"
   ],
   "id": "6ec45f80e0323652",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T06:18:42.134302Z",
     "start_time": "2025-03-07T06:18:42.125034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define dataset split percentages\n",
    "train_ratio, test_ratio, future_ratio = 0.65, 0.15, 0.20\n",
    "\n",
    "# Compute dataset sizes\n",
    "train_size = int(train_ratio * len(X))\n",
    "test_size = int(test_ratio * len(X))\n",
    "\n",
    "# Split into train, test, and future holdout\n",
    "X_train, X_test, X_future = X[:train_size], X[train_size:train_size+test_size], X[train_size+test_size:]\n",
    "y_train, y_test, y_future = y[:train_size], y[train_size:train_size+test_size], y[train_size+test_size:]\n"
   ],
   "id": "cf444de26f372046",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(64, return_sequences=True, input_shape=(SEQ_LEN, 1)),\n",
    "    tf.keras.layers.LSTM(32),\n",
    "    tf.keras.layers.Dense(PRED_LEN)  # Predict next 240 steps\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    loss=MeanSquaredError(),\n",
    "    optimizer=Adam(learning_rate=0.0001),  # Lower learning rate for stability\n",
    "    metrics=[RootMeanSquaredError()]  # RMSE as evaluation metric\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=\"best_lstm_model.h5\",  # Save model as .h5 file\n",
    "    monitor=\"val_loss\",  # Monitor validation loss\n",
    "    save_best_only=True,  # Save only the best model\n",
    "    save_weights_only=False,  # Save entire model, not just weights\n",
    "    mode=\"min\",  # Lower validation loss is better\n",
    "    verbose=1  # Print a message when saving\n",
    ")\n",
    "\n",
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,  # Stop training if no improvement for 5 epochs\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-07T06:18:42.391417Z",
     "start_time": "2025-03-07T06:18:42.208991Z"
    }
   },
   "id": "e0cb1156f1f55f88",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T06:26:51.552986300Z",
     "start_time": "2025-03-07T06:18:42.480291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "BATCH_SIZE = 256\n",
    "EPOCHS = 10\n",
    "\n",
    "# Convert NumPy arrays into TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "tf.config.optimizer.set_jit(True)  # Enable XLA for faster execution\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback]\n",
    ")\n",
    "\n",
    "model = load_model(\"best_lstm_model.h5\")\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n"
   ],
   "id": "fefcc46063cb4c69",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001B[1m1799/1799\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 83ms/step - loss: 2710358.7500 - root_mean_squared_error: 1645.0183\n",
      "Epoch 1: val_loss improved from inf to 2505033.25000, saving model to best_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1799/1799\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m169s\u001B[0m 91ms/step - loss: 2710407.2500 - root_mean_squared_error: 1645.0336 - val_loss: 2505033.2500 - val_root_mean_squared_error: 1582.7297\n",
      "Epoch 2/10\n",
      "\u001B[1m1799/1799\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 95ms/step - loss: 2685209.2500 - root_mean_squared_error: 1637.3258\n",
      "Epoch 2: val_loss improved from 2505033.25000 to 2486259.00000, saving model to best_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1799/1799\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m186s\u001B[0m 103ms/step - loss: 2685259.0000 - root_mean_squared_error: 1637.3417 - val_loss: 2486259.0000 - val_root_mean_squared_error: 1576.7876\n",
      "Epoch 3/10\n",
      "\u001B[1m1142/1799\u001B[0m \u001B[32m━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━\u001B[0m \u001B[1m1:06\u001B[0m 102ms/step - loss: 2595663.7500 - root_mean_squared_error: 1609.3834"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-07T05:45:58.290715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_test_inv = scaler.inverse_transform(y_test)\n",
    "y_pred_inv = scaler.inverse_transform(y_pred)\n"
   ],
   "id": "a9a08842c2830568",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test_inv[0], label=\"Actual\")\n",
    "plt.plot(y_pred_inv[0], label=\"Predicted\", linestyle=\"dashed\")\n",
    "plt.xlabel(\"Time Steps (3-minute intervals)\")\n",
    "plt.ylabel(\"Energy Consumption\")\n",
    "plt.legend()\n",
    "plt.title(\"TensorFlow LSTM Forecasting: Next 12 Hours\")\n",
    "plt.show()\n"
   ],
   "id": "14b7efa1288c06b8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T05:07:40.302011Z",
     "start_time": "2025-03-07T05:07:40.285049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_metrics(data=reuslts_df, real=\"real_consumption\", predicted=\"predicted_consumption_new\")\n",
    "\n",
    "data_metrics(data=reuslts_df, real=\"real_consumption\", predicted=\"predicted_consumption\")"
   ],
   "id": "9c783be5217dfee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 28.3496\n",
      "MSE: 1449.8315\n",
      "RMSE: 38.0767\n",
      "R²: 0.9768\n",
      "MAE: 83.3045\n",
      "MSE: 8800.2009\n",
      "RMSE: 93.8094\n",
      "R²: 0.8594\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5fecb5658323033d",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

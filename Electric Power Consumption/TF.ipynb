{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "import plotly.express as px\n",
    "from pathlib import Path\n",
    "import holidays\n"
   ],
   "id": "a2fa202e3185003a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def upload_data():\n",
    "    data_path = str(Path.cwd().parent) + \"\\\\Data\\\\EPC\\\\Power Consumption Data.csv\"\n",
    "\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    df = df[df[\"real_consumption\"] > 0]\n",
    "    df = df[df['real_consumption'] <= df['real_consumption'].mean() + 4 * df['real_consumption'].std()]\n",
    "\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df = df.sort_values(by='time',ascending=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def data_metrics(data, real, predicted):\n",
    "\n",
    "    y_true = data[real]\n",
    "    y_pred = data[predicted]\n",
    "\n",
    "\n",
    "    def r2_score_tf(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Computes the R² (coefficient of determination) score in TensorFlow.\n",
    "        \n",
    "        Parameters:\n",
    "        y_true : Tensor\n",
    "            Ground truth values.\n",
    "        y_pred : Tensor\n",
    "            Predicted values.\n",
    "    \n",
    "        Returns:\n",
    "        Tensor\n",
    "            The R² score.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        y_true = tf.convert_to_tensor(y_true, dtype=tf.float32)\n",
    "        y_pred = tf.convert_to_tensor(y_pred, dtype=tf.float32)\n",
    "    \n",
    "        unexplained_error = tf.reduce_sum(tf.square(y_true - y_pred))  # SSE\n",
    "        total_error = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true)))  # SST\n",
    "    \n",
    "        r_squared = 1.0 - (unexplained_error / (total_error + tf.keras.backend.epsilon()))  # Prevent division by zero\n",
    "        return r_squared\n",
    "\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae = tf.keras.losses.MeanAbsoluteError()(y_true, y_pred).numpy()\n",
    "    mse = tf.keras.losses.MeanSquaredError()(y_true, y_pred).numpy()\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score_tf(y_true, y_pred)\n",
    "\n",
    "\n",
    "    # MAE (Mean Absolute Error):\n",
    "    # Lower values are better; good MAE depends on the scale of 'real_consumption'.\n",
    "    # As a rule of thumb, MAE should be significantly smaller than the mean of the target variable.\n",
    "    # Lower is better. Ideally, MAE should be much less than the average value of y_true.\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "\n",
    "    # MSE (Mean Squared Error):\n",
    "    # Similar to MAE but penalizes large errors more heavily. A smaller MSE is better.\n",
    "    # Compare MSE to the variance of 'real_consumption' for context.\n",
    "    # Lower is better. MSE should ideally be close to zero relative to the variance of y_true.\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "    # RMSE (Root Mean Squared Error):\n",
    "    # RMSE is the square root of MSE and is in the same units as 'real_consumption'.\n",
    "    # A good RMSE is often close to the standard deviation of 'real_consumption'.\n",
    "    # Lower is better. RMSE should be comparable to or less than the standard deviation of y_true.\"\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "    # R² (Coefficient of Determination):\n",
    "    # R² measures how well the predictions explain the variability of the data.\n",
    "    # Values close to 1.0 are excellent, indicating the model explains most of the variance.\n",
    "    # Negative values indicate poor fit.\n",
    "    # Closer to 1.0 is better. Values > 0.7 are generally good; < 0.5 indicates underfitting.\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "\n",
    "def feature_engineering(data):\n",
    "\n",
    "    # Extracting basic time-based features\n",
    "    data['hour'] = data['time'].dt.hour  # Hour of the day\n",
    "    data['minute'] = data['time'].dt.minute  # Minute\n",
    "    data['day_of_week'] = data['time'].dt.dayofweek  # Day of the week (0=Monday, 6=Sunday)\n",
    "    data['is_weekend'] = data['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)  # Weekend flag\n",
    "    data['day_of_month'] = data['time'].dt.day\n",
    "    data['week_of_year'] = data['time'].dt.isocalendar().week\n",
    "    data['month'] = data['time'].dt.month\n",
    "    data['quarter'] = data['time'].dt.quarter\n",
    "    data['year'] = data['time'].dt.year\n",
    "\n",
    "\n",
    "    # Generate lag features for temporal dependency modeling\n",
    "    for lag in range(1, 5):  # Create lag features for the past 4 time steps\n",
    "        data[f'lag_{lag}'] = data['real_consumption'].shift(lag)\n",
    "\n",
    "\n",
    "    # Generate exponential moving averages\n",
    "    for span in [3, 5]:  # Spans of size 3, 5, and 7\n",
    "        data[f'ema_{span}'] = data['real_consumption'].ewm(span=span, adjust=False).mean()\n",
    "\n",
    "    # Rolling average over a longer period (e.g., weekly and monthly moving averages)\n",
    "    data['weekly_avg'] = data['real_consumption'].rolling(window=7*24*20, min_periods=1).mean()  # Weekly moving avg\n",
    "    data['monthly_avg'] = data['real_consumption'].rolling(window=30*24*20, min_periods=1).mean()  # Monthly moving avg\n",
    "\n",
    "\n",
    "    # Percentage change in real consumption\n",
    "    data['pct_change'] = data['real_consumption'].pct_change()\n",
    "\n",
    "    data['hour_sin'] = np.sin(2 * np.pi * data['hour'] / 24)  # Cyclic hour feature (sine)\n",
    "    data['hour_cos'] = np.cos(2 * np.pi * data['hour'] / 24)  # Cyclic hour feature (cosine)\n",
    "\n",
    "    data['day_of_week_sin'] = np.sin(2 * np.pi * data['day_of_week'] / 7)\n",
    "    data['day_of_week_cos'] = np.cos(2 * np.pi * data['day_of_week'] / 7)\n",
    "\n",
    "    data['month_sin'] = np.sin(2 * np.pi * data['month'] / 12)\n",
    "    data['month_cos'] = np.cos(2 * np.pi * data['month'] / 12)\n",
    "\n",
    "    data['week_of_year_sin'] = np.sin(2 * np.pi * data['week_of_year'] / 52)\n",
    "    data['week_of_year_cos'] = np.cos(2 * np.pi * data['week_of_year'] / 52)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Get Georgia holidays for all years in the dataset\n",
    "    georgia_holidays = holidays.Georgia(years=range(data[\"year\"].min(), data[\"year\"].max() + 1))\n",
    "\n",
    "    data[\"date\"] = data[\"time\"].dt.date\n",
    "\n",
    "    # Create holiday feature (1 if it's a holiday, 0 otherwise)\n",
    "    data['is_holiday'] = data[\"date\"].map(lambda x: 1 if x in georgia_holidays else 0)\n",
    "\n",
    "    # Add features for the day before and after a holiday\n",
    "    data['is_day_before_holiday'] = data[\"date\"].map(lambda x: 1 if (x - pd.Timedelta(days=1)) in georgia_holidays else 0)\n",
    "    data['is_day_after_holiday'] = data[\"date\"].map(lambda x: 1 if (x + pd.Timedelta(days=1)) in georgia_holidays else 0)\n",
    "\n",
    "\n",
    "\n",
    "    return  data\n",
    "\n"
   ],
   "id": "5a8ca2c473c7e3c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = upload_data()\n",
    "\n",
    "data_metrics(data=df, real=\"real_consumption\", predicted=\"predicted_consumption\")\n",
    "\n",
    "df = feature_engineering(df)\n"
   ],
   "id": "221244da3a5ee410",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df.index = pd.to_datetime(df['time'], format='%d.%m.%Y %H:%M:%S')\n",
    "temp = df['real_consumption']\n",
    "temp"
   ],
   "id": "fbea4b7ff427582a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# [[[1], [2], [3], [4], [5]]] [6]\n",
    "# [[[2], [3], [4], [5], [6]]] [7]\n",
    "# [[[3], [4], [5], [6], [7]]] [8]\n",
    "\n",
    "def df_to_X_y(df, window_size=5):\n",
    "    df_as_np = df.to_numpy()\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(df_as_np)-window_size):\n",
    "        row = [[a] for a in df_as_np[i:i+window_size]]\n",
    "        X.append(row)\n",
    "        label = df_as_np[i+window_size]\n",
    "        y.append(label)\n",
    "    return np.array(X), np.array(y)"
   ],
   "id": "e523bfbe2a7649f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "WINDOW_SIZE = 5\n",
    "X1, y1 = df_to_X_y(temp, WINDOW_SIZE)\n",
    "X1.shape, y1.shape"
   ],
   "id": "96046be896c8da13",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_train1, y_train1 = X1[:600000], y1[:600000]\n",
    "X_val1, y_val1 = X1[600000:650000], y1[600000:650000]\n",
    "X_test1, y_test1 = X1[650000:], y1[650000:]\n",
    "X_train1.shape, y_train1.shape, X_val1.shape, y_val1.shape, X_test1.shape, y_test1.shape"
   ],
   "id": "95cc7235efb0f37d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model1 = Sequential()\n",
    "model1.add(InputLayer((WINDOW_SIZE, 1)))\n",
    "model1.add(LSTM(64))\n",
    "model1.add(Dense(8, 'relu'))\n",
    "model1.add(Dense(1, 'linear'))\n",
    "\n",
    "model1.summary()"
   ],
   "id": "cf32fc7ebe3ba5d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "\n",
    "# Define model checkpoint path with a valid .keras filename\n",
    "checkpoint_path = str(Path.cwd().parent / \"Data\" / \"best_model.keras\")\n",
    "\n",
    "cp1 = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,  # Ensure it ends with .keras\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "model1.compile(\n",
    "    loss=MeanSquaredError(),\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    metrics=[RootMeanSquaredError()]\n",
    ")\n"
   ],
   "id": "13be61e93837a924",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model1.fit(X_train1, y_train1, validation_data=(X_val1, y_val1), epochs=20, callbacks=[cp1])",
   "id": "436889acb26c564c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m18750/18750\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m109s\u001B[0m 6ms/step - loss: 322.2142 - root_mean_squared_error: 17.9500 - val_loss: 327.5891 - val_root_mean_squared_error: 18.0994\n",
      "Epoch 19/20\n",
      "\u001B[1m18750/18750\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m111s\u001B[0m 6ms/step - loss: 331.1414 - root_mean_squared_error: 18.1855 - val_loss: 340.5874 - val_root_mean_squared_error: 18.4550\n",
      "Epoch 20/20\n",
      "\u001B[1m18750/18750\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m112s\u001B[0m 6ms/step - loss: 321.9810 - root_mean_squared_error: 17.9437 - val_loss: 366.0849 - val_root_mean_squared_error: 19.1333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x17184714b10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T06:53:52.768583600Z",
     "start_time": "2025-03-07T06:52:39.353042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model1 = load_model(checkpoint_path)\n",
    "\n",
    "train_predictions = model1.predict(X_train1).flatten()\n",
    "train_results = pd.DataFrame(data={'Train Predictions':train_predictions, 'Actuals':y_train1})\n",
    "\n",
    "val_predictions = model1.predict(X_val1).flatten()\n",
    "val_results = pd.DataFrame(data={'Val Predictions':val_predictions, 'Actuals':y_val1})\n",
    "\n",
    "test_predictions = model1.predict(X_test1).flatten()\n",
    "test_results = pd.DataFrame(data={'Test Predictions':test_predictions, 'Actuals':y_test1})\n"
   ],
   "id": "9989dcc18dad1ae9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m18750/18750\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m48s\u001B[0m 3ms/step\n",
      "\u001B[1m1563/1563\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 3ms/step\n",
      "\u001B[1m1475/1823\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m1s\u001B[0m 3ms/step"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_results[\"index\"] = temp[650005:].index\n",
    "test_results"
   ],
   "id": "7c592e82e5ba4be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "predict_points = 100\n",
    "test_predictions = []\n",
    "last_several_month = temp[650005:650011].copy()\n",
    "\n",
    "\n",
    "current_batch = df_to_X_y(last_several_month, window_size=5)[0]\n",
    "\n",
    "for i in range(predict_points):\n",
    "\n",
    "    # get the prediction value for the first batch\n",
    "    current_pred = model1.predict(current_batch).flatten()\n",
    "\n",
    "    # append the prediction into the array\n",
    "    test_predictions.append(current_pred)\n",
    "\n",
    "    # use the prediction to update the batch and remove the first value\n",
    "    current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "results = temp[650010:(650010 + predict_points)].copy().reset_index()\n",
    "results[\"predic\"] = pd.DataFrame(test_predictions).rename(columns={0:\"predict\"})[\"predict\"]\n",
    "results"
   ],
   "id": "f367fe2844015489",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create an interactive plot using Plotly\n",
    "fig = px.line(results, x='time', y=['real_consumption', 'predic'], title=\"Interactive Time Series Plot\")\n",
    "\n",
    "# Customize the plot\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Timestamp\",\n",
    "    yaxis_title=\"Values\",\n",
    "    legend_title=\"Legend\",\n",
    "    xaxis_rangeslider_visible=True  # Enables the date slicer\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ],
   "id": "8dfa2d39d849e207",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data_metrics(data=test_results, real=\"Actuals\", predicted=\"Test Predictions\")",
   "id": "97a81b223a2498ab",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
